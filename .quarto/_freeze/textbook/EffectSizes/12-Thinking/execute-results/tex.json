{
  "hash": "fd1ec3db025997deb3a9441cb89d6214",
  "result": {
    "markdown": "---\nauthor: \n  - Matthew J. C. Crump\naliases: [thinking-about-answering-questions-with-data.html]\n---\n\n\n\n\n\n# Thinking about answering questions with data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nlibrary(dplyr)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'dplyr'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n```\n:::\n:::\n\n\n\nYou might be happy that this is the last chapter (so far) of this\ntextbook. At this point we are in the last weeks of our introductory\nstatistics course. It's called \"introductory\" for a reason. There's just\ntoo much out there to cover in one short semester. In this chapter we\nacknowledge some of the things we haven't yet covered, and treat them as\nthings that you should think about. If there is one take home message\nthat we want to get across to you, it's that when you ask questions with\ndata, you should be able to **justify** how you answer those questions.\n\n## Effect-size and power\n\nIf you already know something about statistics while you were reading\nthis book, you might have noticed that we neglected to discuss the topic\nof effect-size, and we barely talked about statistical power. We will\ntalk a little bit about these things here.\n\nFirst, it is worth pointing out that over the years, at least in\nPsychology, many societies and journals have made recommendations about\nhow researchers should report their statistical analyses. Among the\nrecommendations is that measures of \"effect size\" should be reported.\nSimilarly, many journals now require that researchers report an \"a\npriori\" power-analysis (the recommendation is this should be done before\nthe data is collected). Because these recommendations are so prevalent,\nit is worth discussing what these ideas refer to. At the same time, the\nmeaning of effect-size and power somewhat depend on your \"philosophical\"\nbent, and these two ideas can become completely meaningless depending on\nhow you think of statistics. For these complicating reasons we have\nsuspended our discussion of the topic until now.\n\nThe question or practice of using measures of effect size and conducting\npower-analyses are also good examples of the more general need to think\nabout about what you are doing. If you are going to report effect size,\nand conduct power analyses, these activities should not be done blindly\nbecause someone else recommends that you do them, these activities and\nother suitable ones should be done as a part of justifying what you are\ndoing. It is a part of thinking about how to make your data answer\nquestions for you.\n\n### Chance vs. real effects\n\nLet's rehash something we've said over and over again. First,\nresearchers are interested in whether their manipulation causes a change\nin their measurement. If it does, they can become confident that they\nhave uncovered a causal force (the manipulation). However, we know that\ndifferences in the measure between experimental conditions can arise by\nchance alone, just by sampling error. In fact, we can create pictures\nthat show us the window of chance for a given statistic, these tells us\nroughly the range and likelihoods of getting various differences just by\nchance. With these windows in hand, we can then determine whether the\ndifferences we found in some data that we collected were likely or\nunlikely to be due to chance. We also learned that sample-size plays a\nbig role in the shape of the chance window. Small samples give chance a\nlarge opportunity make big differences. Large samples give chance a\nsmall opportunity to make big differences. The general lesson up to this\npoint has been, design an experiment with a large enough sample to\ndetect the effect of interest. If your design isn't well formed, you\ncould easily be measuring noise, and your differences could be caused by\nsampling error. Generally speaking, this is still a very good lesson:\nbetter designs produce better data; and you can't fix a broken design\nwith statistics.\n\nThere is clearly another thing that can determine whether or not your\ndifferences are due to chance. That is the effect itself. If the\nmanipulation does cause a change, then there is an effect, and that\neffect is a real one. Effects refer to differences in the measurement\nbetween experimental conditions. The thing about effects is that they\ncan be big or small, they have a size.\n\nFor example, you can think of a manipulation in terms of the size of its\nhammer. A strong manipulation is like a jack-hammer: it is loud, it\nproduces a big effect, it creates huge differences. A medium\nmanipulation is like regular hammer: it works, you can hear it, it\ndrives a nail into wood, but it doesn't destroy concrete like a\njack-hammer, it produces a reliable effect. A small manipulation is like\ntapping something with a pencil: it does something, you can barely hear\nit, and only in a quiet room, it doesn't do a good job of driving a nail\ninto wood, and it does nothing to concrete, it produces tiny, unreliable\neffects. Finally, a really small effect would be hammering something\nwith a feather, it leaves almost no mark and does nothing that is\nobviously perceptiple to nails or pavement. The lesson is, if you want\nto break up concrete, use a jack-hammer; or, if you want to measure your\neffect, make your manipulation stronger (like a jack-hammer) so it\nproduces a bigger difference.\n\n### Effect size: concrete vs. abstract notions\n\nGenerally speaking, the big concept of effect size, is simply how big\nthe differences are, that's it. However, the biggness or smallness of\neffects quickly becomes a little bit complicated. On the one hand, the\nraw difference in the means can be very meaningful. Let's saw we are\nmeasuring performance on a final exam, and we are testing whether or not\na miracle drug can make you do better on the test. Let's say taking the\ndrug makes you do 5% better on the test, compared to not taking the\ndrug. You know what 5% means, that's basically a whole letter grade.\nPretty good. An effect-size of 25% would be even better right! Lot's of\nmeasures have a concrete quality to them, and we often want to the size\nof the effect expressed in terms of the original measure.\n\nLet's talk about concrete measures some more. How about learning a\nmusical instrument. Let's say it takes 10,000 hours to become an expert\npiano, violin, or guitar player. And, let's say you found something\nonline that says that using their method, you will learn the instrument\nin less time than normal. That is a claim about the effect size of their\nmethod. You would want to know how big the effect is right? For example,\nthe effect-size could be 10 hours. That would mean it would take you\n9,980 hours to become an expert (that's a whole 10 hours less). If I\nknew the effect-size was so tiny, I wouldn't bother with their new\nmethod. But, if the effect size was say 1,000 hours, that's a pretty big\ndeal, that's 10% less (still doesn't seem like much, but saving 1,000\nhours seems like a lot).\n\nJust as often as we have concrete measures that are readily\ninterpretable, Psychology often produces measures that are extremely\ndifficult to interpret. For example, questionnaire measures often have\nno concrete meaning, and only an abstract statistical meaning. If you\nwanted to know whether a manipulation caused people to more or less\nhappy, and you used to questionnaire to measure happiness, you might\nfind that people were 50 happy in condition 1, and 60 happy in condition\n2, that's a difference of 10 happy units. But how much is 10? Is that a\nbig or small difference? It's not immediately obvious. What is the\nsolution here? A common solution is to provide a standardized measure of\nthe difference, like a z-score. For example, if a difference of 10\nreflected a shift of one standard deviation that would be useful to\nknow, and that would be a sizeable shift. If the difference was only a\n.1 shift in terms of standard deviation, then the difference of 10\nwouldn't be very large. We elaborate on this idea next in describing\ncohen's d.\n\n### Cohen's d\n\nLet's look a few distributions to firm up some ideas about effect-size.\n@fig-12effectdists has four panels. The first panel (0) represents the\nnull distribution of no differences. This is the idea that your\nmanipulation (A vs. B) doesn't do anything at all, as a result when you\nmeasure scores in conditions A and B, you are effectively sampling\nscores from the very same overall distribution. The panel shows the\ndistribution as green for condition B, but the red one for condition A\nis identical and drawn underneath (it's invisible). There is 0\ndifference between these distributions, so it represent a null effect.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nX <- c(\n  seq(-5, 5, .1),\n  seq(-5, 5, .1),\n  seq(-5, 5, .1),\n  seq(-5, 5, .1),\n  seq(-5, 5, .1),\n  seq(-5, 5, .1),\n  seq(-5, 5, .1),\n  seq(-5, 5, .1)\n)\nY <- c(\n  dnorm(seq(-5, 5, .1), 0, 1),\n  dnorm(seq(-5, 5, .1), 0, 1),\n  dnorm(seq(-5, 5, .1), 0, 1),\n  dnorm(seq(-5, 5, .1), .5, 1),\n  dnorm(seq(-5, 5, .1), 0, 1),\n  dnorm(seq(-5, 5, .1), 1, 1),\n  dnorm(seq(-5, 5, .1), 0, 1),\n  dnorm(seq(-5, 5, .1), 2, 1)\n)\neffect_size <- rep(c(0, .5, 1, 2), each = 101 * 2)\ncondition <- rep(rep(c(\"A\", \"B\"), each = 101), 2)\ndf <- data.frame(effect_size,\n                 condition,\n                 X, Y)\n\nggplot(df, aes(\n  x = X,\n  y = Y,\n  group = condition,\n  color = condition\n)) +\n  geom_line() +\n  theme_classic(base_size = 15) +\n  facet_wrap( ~ effect_size) +\n  xlab(\"values\") +\n  ylab(\"density\")\n```\n\n::: {.cell-output-display}\n![Each panel shows hypothetical distributions for two conditions. As the effect-size increases, the difference between the distributions become larger.](12-Thinking_files/figure-pdf/fig-12effectdists-1.pdf){#fig-12effectdists fig-pos='H' width=100%}\n:::\n:::\n\n\n\nThe remaining panels are hypothetical examples of what a true effect\ncould look like, when your manipulation actually causes a difference.\nFor example, if condition A is a control group, and condition B is a\ntreatment group, we are looking at three cases where the treatment\nmanipulation causes a positive shift in the mean of distribution. We are\nusing normal curves with mean =0 and sd =1 for this demonstration, so a\nshift of .5 is a shift of half of a standard deviation. A shift of 1 is\na shift of 1 standard deviation, and a shift of 2 is a shift of 2\nstandard deviations. We could draw many more examples showing even\nbigger shifts, or shifts that go in the other direction.\n\nLet's look at another example, but this time we'll use some concrete\nmeasurements. Let's say we are looking at final exam performance, so our\nnumbers are grade percentages. Let's also say that we know the mean on\nthe test is 65%, with a standard deviation of 5%. Group A could be a\ncontrol that just takes the test, Group B could receive some\n\"educational\" manipulation designed to improve the test score. These\ngraphs then show us some hypotheses about what the manipulation may or\nmay not be doing.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nX <- c(\n  seq(25, 100, 1),\n  seq(25, 100, 1),\n  seq(25, 100, 1),\n  seq(25, 100, 1),\n  seq(25, 100, 1),\n  seq(25, 100, 1),\n  seq(25, 100, 1),\n  seq(25, 100, 1)\n)\nY <- c(\n  dnorm(seq(25, 100, 1), 65, 5),\n  dnorm(seq(25, 100, 1), 65, 5),\n  dnorm(seq(25, 100, 1), 65, 5),\n  dnorm(seq(25, 100, 1), 67.5, 5),\n  dnorm(seq(25, 100, 1), 65, 5),\n  dnorm(seq(25, 100, 1), 70, 5),\n  dnorm(seq(25, 100, 1), 65, 5),\n  dnorm(seq(25, 100, 1), 75, 5)\n)\neffect_size <-\n  rep(c(\"65, d=0\", \"67.5,d=.5\", \"70, d=1\", \"75, d=2\"), each = 76 * 2)\ncondition <- rep(rep(c(\"A\", \"B\"), each = 76), 2)\ndf <- data.frame(effect_size,\n                 condition,\n                 X, Y)\n\nggplot(df, aes(\n  x = X,\n  y = Y,\n  group = condition,\n  color = condition\n)) +\n  geom_line() +\n  theme_classic(base_size = 15) +\n  facet_wrap( ~ effect_size) +\n  xlab(\"values\") +\n  ylab(\"density\")\n```\n\n::: {.cell-output-display}\n![Each panel shows hypothetical distributions for two conditions. As the effect-size increases, the difference between the distributions become larger.](12-Thinking_files/figure-pdf/fig-12effectdistsB-1.pdf){#fig-12effectdistsB fig-pos='H' width=100%}\n:::\n:::\n\n\n\nThe first panel shows that both condition A and B will sample test\nscores from the same distribution (mean =65, with 0 effect). The other\npanels show shifted mean for condition B (the treatment that is supposed\nto increase test performance). So, the treatment could increase the test\nperformance by 2.5% (mean 67.5, .5 sd shift), or by 5% (mean 70, 1 sd\nshift), or by 10% (mean 75%, 2 sd shift), or by any other amount. In\nterms of our previous metaphor, a shift of 2 standard deviations is more\nlike jack-hammer in terms of size, and a shift of .5 standard deviations\nis more like using a pencil. The thing about research, is we often have\nno clue about whether our manipulation will produce a big or small\neffect, that's why we are conducting the research.\n\nYou might have noticed that the letter $d$ appears in the above figure.\nWhy is that? Jacob Cohen [@cohen1988] used the letter $d$ in defining\nthe effect-size for this situation, and now everyone calls it Cohen's\n$d$. The formula for Cohen's $d$ is:\n\n$d = \\frac{\\text{mean for condition 1} - \\text{mean for condition 2}}{\\text{population standard deviation}}$\n\nIf you notice, this is just a kind of z-score. It is a way to\nstandardize the mean difference in terms of the population standard\ndeviation.\n\nIt is also worth noting again that this measure of effect-size is\nentirely hypothetical for most purposes. In general, researchers do not\nknow the population standard deviation, they can only guess at it, or\nestimate it from the sample. The same goes for means, in the formula\nthese are hypothetical mean differences in two population distributions.\nIn practice, researchers do not know these values, they guess at them\nfrom their samples.\n\nBefore discussing why the concept of effect-size can be useful, we note\nthat Cohen's $d$ is useful for understanding abstract measures. For\nexample, when you don't know what a difference of 10 or 20 means as a\nraw score, you can standardize the difference by the sample standard\ndeviation, then you know roughly how big the effect is in terms of\nstandard units. If you thought a 20 was big, but it turned out to be\nonly 1/10th of a standard deviation, then you would know the effect is\nactually quite small with respect to the overall variability in the\ndata.\n\n## Power\n\nWhen there is a true effect out there to measure, you want to make sure\nyour design is sensitive enough to detect the effect, otherwise what's\nthe point. We've already talked about the idea that an effect can have\ndifferent sizes. The next idea is that your design can be more less\nsensitive in its ability to reliabily measure the effect. We have\ndiscussed this general idea many times already in the textbook, for\nexample we know that we will be more likely to detect \"significant\"\neffects (when there are real differences) when we increase our\nsample-size. Here, we will talk about the idea of design sensitivity in\nterms of the concept of power. Interestingly, the concept of power is a\nsomewhat limited concept, in that it only exists as a concept within\nsome philosophies of statistics.\n\n### A digresssion about hypothesis testing\n\nIn particular, the concept of power falls out of the Neyman-Pearson\nconcept of null vs. alternative hypothesis testing. Up to this point, we\nhave largely avoided this terminology. This is perhaps a disservice in\nthat the Neyman-Pearson ideas are by now the most common and widespread,\nand in the opinion of some of us, they are also the most widely\nmisunderstood and abused idea, which is why we have avoided these ideas\nuntil now.\n\nWhat we have been mainly doing is talking about hypothesis testing from\nthe Fisherian (Sir Ronald Fisher, the ANOVA guy) perspective. This is a\nbasic perspective that we think can't be easily ignored. It is also\nquite limited. The basic idea is this:\n\n1.  We know that chance can cause some differences when we measure\n    something between experimental conditions.\n2.  We want to rule out the possibility that the difference that we\n    observed can not be due to chance\n3.  We construct large N designs that permit us to do this when a real\n    effect is observed, such that we can confidently say that big\n    differences that we find are so big (well outside the chance window)\n    that it is highly implausible that chance alone could have produced.\n4.  The final conclusion is that chance was extremely unlikely to have\n    produced the differences. We then infer that something else, like\n    the manipulation, must have caused the difference.\n5.  We don't say anything else about the something else.\n6.  We either reject the null distribution as an explanation (that\n    chance couldn't have done it), or retain the null (admit that chance\n    could have done it, and if it did we couldn't tell the difference\n    between what we found and what chance could do)\n\nNeyman and Pearson introduced one more idea to this mix, the idea of an\nalternative hypothesis. The alternative hypothesis is the idea that if\nthere is a true effect, then the data sampled into each condition of the\nexperiment must have come from two different distributions. Remember,\nwhen there is no effect we assume all of the data cam from the same\ndistribution (which by definition can't produce true differences in the\nlong run, because all of the numbers are coming from the same\ndistribution). The graphs of effect-sizes from before show examples of\nthese alternative distributions, with samples for condition A coming\nfrom one distribution, and samples from condition B coming from a\nshifted distribution with a different mean.\n\nSo, under the Neyman-Pearson tradition, when a researcher find a\nsignifcant effect they do more than one things. First, they reject the\nnull-hypothesis of no differences, and they accept the alternative\nhypothesis that there was differences. This seems like a sensible thing\nto do. And, because the researcher is actually interested in the\nproperties of the real effect, they might be interested in learning more\nabout the actual alternative hypothesis, that is they might want to know\nif their data come from two different distributions that were separated\nby some amount...in other words, they would want to know the size of the\neffect that they were measuring.\n\n### Back to power\n\nWe have now discussed enough ideas to formalize the concept of\nstatistical power. For this concept to exist we need to do a couple\nthings.\n\n1.  Agree to set an alpha criterion. When the p-value for our\n    test-statistic is below this value we will call our finding\n    statistically significant, and agree to reject the null hypothesis\n    and accept the \"alternative\" hypothesis (sidenote, usually it isn't\n    very clear which specific alternative hypothesis was accepted)\n2.  In advance of conducting the study, figure out what kinds of\n    effect-sizes our design is capable of detecting with particular\n    probabilites.\n\nThe power of a study is determined by the relationship between\n\n1.  The sample-size of the study\n2.  The effect-size of the manipulation\n3.  The alpha value set by the researcher.\n\nTo see this in practice let's do a simulation. We will do a t-test on a\nbetween-groups design 10 subjects in each group. Group A will be a\ncontrol group with scores sampled from a normal distribution with mean\nof 10, and standard deviation of 5. Group B will be a treatment group,\nwe will say the treatment has an effect-size of Cohen's $d$ = .5, that's\na standard deviation shift of .5, so the scores with come from a normal\ndistribution with mean =12.5 and standard deivation of 5. Remember 1\nstandard deviation here is 5, so half of a standard deviation is 2.5.\n\nThe following R script runs this simulated experiment 1000 times. We set\nthe alpha criterion to .05, this means we will reject the null whenever\nthe $p$-value is less than .05. With this specific design, how many\ntimes out of of 1000 do we reject the null, and accept the alternative\nhypothesis?\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\np<-length(1000)\nfor(i in 1:1000){\n  A<-rnorm(10,10,5)\n  B<-rnorm(10,12.5,5)\n  p[i]<-t.test(A,B,var.equal = TRUE)$p.value\n}\n\nlength(p[p<.05])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 179\n```\n:::\n:::\n\n\n\nThe answer is that we reject the null, and accept the alternative\n179 times out of 1000. In other words our experiment\nsuccesfully accepts the alternative hypothesis\n17.9 percent of the time, this is known as the\npower of the study. Power is the probability that a design will\nsuccesfully detect an effect of a specific size.\n\nImportantly, power is completely abstract idea that is completely\ndetermined by many assumptions including N, effect-size, and alpha. As a\nresult, it is best not to think of power as a single number, but instead\nas a family of numbers.\n\nFor example, power is different when we change N. If we increase N, our\nsamples will more precisely estimate the true distributions that they\ncame from. Increasing N reduces sampling error, and shrinks the range of\ndifferences that can be produced by chance. Lets' increase our N in this\nsimulation from 10 to 20 in each group and see what happens.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\np<-length(1000)\nfor(i in 1:1000){\n  A<-rnorm(20,10,5)\n  B<-rnorm(20,12.5,5)\n  p[i]<-t.test(A,B,var.equal = TRUE)$p.value\n}\n\nlength(p[p<.05])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 348\n```\n:::\n:::\n\n\n\nNow the number of significant experiments i 348 out of\n1000, or a power of 34.8 percent. That's\nroughly doubled from before. We have made the design more sensitive to\nthe effect by increasing N.\n\nWe can change the power of the design by changing the alpha-value, which\ntells us how much evidence we need to reject the null. For example, if\nwe set the alpha criterion to 0.01, then we will be more conservative,\nonly rejecting the null when chance can produce the observed difference\n1% of the time. In our example, this will have the effect of reducing\npower. Let's keep N at 20, but reduce the alpha to 0.01 and see what\nhappens:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\np<-length(1000)\nfor(i in 1:1000){\n  A<-rnorm(20,10,5)\n  B<-rnorm(20,12.5,5)\n  p[i]<-t.test(A,B,var.equal = TRUE)$p.value\n}\n\nlength(p[p<.01])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 153\n```\n:::\n:::\n\n\n\nNow only 153 out of 1000 experiments are significant,\nthat's 15.3 power.\n\nFinally, the power of the design depends on the actual size of the\neffect caused by the manipulation. In our example, we hypothesized that\nthe effect caused a shift of .5 standard deviations. What if the effect\ncauses a bigger shift? Say, a shift of 2 standard deviations. Let's keep\nN= 20, and alpha \\< .01, but change the effect-size to two standard\ndeviations. When the effect in the real-world is bigger, it should be\neasier to measure, so our power will increase.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\np<-length(1000)\nfor(i in 1:1000){\n  A<-rnorm(20,10,5)\n  B<-rnorm(20,30,5)\n  p[i]<-t.test(A,B,var.equal = TRUE)$p.value\n}\n\nlength(p[p<.01])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1000\n```\n:::\n:::\n\n\n\nNeat, if the effect-size is actually huge (2 standard deviation shift),\nthen we have power 100 percent to detect the\ntrue effect.\n\n### Power curves\n\nWe mentioned that it is best to think of power as a family of numbers,\nrather than as a single number. To elaborate on this consider the power\ncurve below. This is the power curve for a specific design: a between\ngroups experiments with two levels, that uses an independent samples\nt-test to test whether an observed difference is due to chance.\nCritically, N is set to 10 in each group, and alpha is set to .05\n\nIn @fig-12powercurve power (as a proportion, not a percentage) is\nplotted on the y-axis, and effect-size (Cohen's d) in standard deviation\nunits is plotted on the x-axis.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npower<-c()\nfor(i in seq(0,2,.1)){\nsd_AB <- 1\nn<-10\nC <- qnorm(0.975)\nse <- sqrt( sd_AB/n + sd_AB/n )\ndelta<-i\npower <- c(power,1-pnorm(C-delta/se) + pnorm(-C-delta/se))\n}\n\nplot_df<-data.frame(power,\n                    effect_size = seq(0,2,.1))\n\nggplot(plot_df, aes(x=effect_size, y=power))+\n  geom_line()+\n  theme_classic()\n```\n\n::: {.cell-output-display}\n![This figure shows power as a function of effect-size (Cohen's d) for a between-subjects independent samples t-test, with N=10, and alpha criterion 0.05.](12-Thinking_files/figure-pdf/fig-12powercurve-1.pdf){#fig-12powercurve fig-pos='H' width=100%}\n:::\n:::\n\n\n\nA power curve like this one is very helpful to understand the\nsensitivity of a particular design. For example, we can see that a\nbetween subjects design with N=10 in both groups, will detect an effect\nof d=.5 (half a standard deviation shift) about 20% of the time, will\ndetect an effect of d=.8 about 50% of the time, and will detect an\neffect of d=2 about 100% of the time. All of the percentages reflect the\npower of the design, which is the percentage of times the design would\nbe expected to find a $p$ \\< 0.05.\n\nLet's imagine that based on prior research, the effect you are\ninterested in measuring is fairly small, d=0.2. If you want to run an\nexperiment that will detect an effect of this size a large percentage of\nthe time, how many subjects do you need to have in each group? We know\nfrom the above graph that with N=10, power is very low to detect an\neffect of d=0.2. Let's make @fig-12powercurveN and vary the number of\nsubjects rather than the size of the effect.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npower<-c()\nfor(i in seq(10,800,10)){\nsd_AB <- 1\nn<-i\nC <- qnorm(0.975)\nse <- sqrt( sd_AB/n + sd_AB/n )\ndelta<-0.2\npower <- c(power,1-pnorm(C-delta/se) + pnorm(-C-delta/se))\n}\n\nplot_df<-data.frame(power,\n                    N = seq(10,800,10))\n\nggplot(plot_df, aes(x=N, y=power))+\n  geom_line()+\n  theme_classic()+\n  geom_hline(yintercept=.8, color=\"green\")\n```\n\n::: {.cell-output-display}\n![This figure shows power as a function of N for a between-subjects independent samples t-test, with d=0.2, and alpha criterion 0.05.](12-Thinking_files/figure-pdf/fig-12powercurveN-1.pdf){#fig-12powercurveN fig-pos='H' width=100%}\n:::\n:::\n\n\n\nThe figure plots power to detect an effect of d=0.2, as a function of N.\nThe green line shows where power = .8, or 80%. It looks like we would\nnee about 380 subjects in each group to measure an effect of d=0.2, with\npower = .8. This means that 80% of our experiments would succesfully\nshow p \\< 0.05. Often times power of 80% is recommended as a reasonable\nlevel of power, however even when your design has power = 80%, your\nexperiment will still fail to find an effect (associated with that level\nof power) 20% of the time!\n\n## Planning your design\n\nOur discussion of effect size and power highlight the importance of the\nunderstanding the statistical limitations of an experimental design. In\nparticular, we have seen the relationship between:\n\n1.  Sample-size\n2.  Effect-size\n3.  Alpha criterion\n4.  Power\n\nAs a general rule of thumb, small N designs can only reliably detect\nvery large effects, whereas large N designs can reliably detect much\nsmaller effects. As a researcher, it is your responsibility to plan your\ndesign accordingly so that it is capable of reliably detecting the kinds\nof effects it is intended to measure.\n\n## Some considerations\n\n### Low powered studies\n\nConsider the following case. A researcher runs a study to detect an\neffect of interest. There is good reason, from prior research, to\nbelieve the effect-size is d=0.5. The researcher uses a design that has\n30% power to detect the effect. They run the experiment and find a\nsignificant p-value, (p\\<.05). They conclude their manipulation worked,\nbecause it was unlikely that their result could have been caused by\nchance. How would you interpret the results of a study like this? Would\nyou agree with thte researchers that the manipulation likely caused the\ndifference? Would you be skeptical of the result?\n\nThe situation above requires thinking about two kinds of probabilities.\nOn the one hand we know that the result observed by the researchers does\nnot occur often by chance (p is less than 0.05). At the same time, we\nknow that the design was underpowered, it only detects results of the\nexpected size 30% of the time. We are face with wondering what kind of\nluck was driving the difference. The researchers could have gotten\nunlucky, and the difference really could be due to chance. In this case,\nthey would be making a type I error (saying the result is real when it\nisn't). If the result was not due to chance, then they would also be\nlucky, as their design only detects this effect 30% of the time.\n\nPerhaps another way to look at this situation is in terms of the\nreplicability of the result. Replicability refers to whether or not the\nfindings of the study would be the same if the experiment was repeated.\nBecause we know that power is low here (only 30%), we would expect that\nmost replications of this experiment would not find a significant\neffect. Instead, the experiment would be expected to replicate only 30%\nof the time.\n\n### Large N and small effects\n\nPerhaps you have noticed that there is an intriguiing relationship\nbetween N (sample-size) and power and effect-size. As N increases, so\ndoes power to detect an effect of a particular size. Additionally, as N\nincreases, a design is capable of detecting smaller and smaller effects\nwith greater and greater power. For example, if N was large enough, we\nwould have high power to detect very small effects, say d= 0.01, or even\nd=0.001. Let's think about what this means.\n\nImagine a drug company told you that they ran an experiment with 1\nbillion people to test whether their drug causes a significant change in\nheadache pain. Let's say they found a significant effect (with power\n=100%), but the effect was very small, it turns out the drug reduces\nheadache pain by less than 1%, let's say 0.01%. For our imaginary study\nwe will also assume that this effect is very real, and not caused by\nchance.\n\nClearly the design had enough power to detect the effect, and the effect\nwas there, so the design did detect the effect. However, the issue is\nthat there is little practical value to this effect. Nobody is going to\nby a drug to reduce their headache pain by 0.01%, even if it was\n\"scientifcally proven\" to work. This example brings up two issues.\nFirst, increasing N to very large levels will allow designs to detect\nalmost any effect (even very tiny ones) with very high power. Second,\nsometimes effects are meaningless when they are very small, especially\nin applied research such as drug studies.\n\nThese two issues can lead to interesting suggestions. For example,\nsomeone might claim that large N studies aren't very useful, because\nthey can always detect really tiny effects that are practically\nmeaningless. On the other hand, large N studies will also detect larger\neffects too, and they will give a better estimate of the \"true\" effect\nin the population (because we know that larger samples do a better job\nof estimating population parameters). Additionally, although really\nsmall effects are often not interesting in the context of applied\nresearch, they can be very important in theoretical research. For\nexample, one theory might predict that manipulating X should have no\neffect, but another theory might predict that X does have an effect,\neven if it is a small one. So, detecting a small effect can have\ntheoretical implication that can help rule out false theories. Generally\nspeaking, researchers asking both theoretical and applied questions\nshould think about and establish guidelines for \"meaningful\"\neffect-sizes so that they can run designs of appropriate size to detect\neffects of \"meaningful size\".\n\n### Small N and Large effects\n\nAll other things being equal would you trust the results from a study\nwith small N or large N? This isn't a trick question, but sometimes\npeople tie themselves into a knot trying to answer it. We already know\nthat large sample-sizes provide better estimates of the distributions\nthe samples come from. As a result, we can safely conclude that we\nshould trust the data from large N studies more than small N studies.\n\nAt the same time, you might try to convince yourself otherwise. For\nexample, you know that large N studies can detect very small effects\nthat are practically and possibly even theoretically meaningless. You\nalso know that that small N studies are only capable of reliably\ndetecting very large effects. So, you might reason that a small N study\nis better than a large N study because if a small N study detects an\neffect, that effect must be big and meaningful; whereas, a large N study\ncould easily detect an effect that is tiny and meaningless.\n\nThis line of thinking needs some improvement. First, just because a\nlarge N study can detect small effects, doesn't mean that it only\ndetects small effects. If the effect is large, a large N study will\neasily detect it. Large N studies have the power to detect a much wider\nrange of effects, from small to large. Second, just because a small N\nstudy detected an effect, does not mean that the effect is real, or that\nthe effect is large. For example, small N studies have more variability,\nso the estimate of the effect size will have more error. Also, there is\n5% (or alpha rate) chance that the effect was spurious. Interestingly,\nthere is a pernicious relationship between effect-size and type I error\nrate\n\n### Type I errors are convincing when N is small\n\nSo what is this pernicious relationship between Type I errors and\neffect-size? Mainly, this relationship is pernicious for small N\nstudies. For example, the following figure illustrates the results of\n1000s of simulated experiments, all assuming the null distribution. In\nother words, for all of these simulations there is no true effect, as\nthe numbers are all sampled from an identical distribution (normal\ndistribution with mean =0, and standard deviation =1). The true\neffect-size is 0 in all cases.\n\nWe know that under the null, researchers will find p values that are\nless 5% about 5% of the time, remember that is the definition. So, if a\nresearcher happened to be in this situation (where there manipulation\ndid absolutely nothing), they would make a type I error 5% of the time,\nor if they conducted 100 experiments, they would expect to find a\nsignificant result for 5 of them.\n\n@fig-12effectsizeType1 reports the findings from only the type I errors,\nwhere the simulated study did produce p \\< 0.05. For each type I error,\nwe calculated the exact p-value, as well as the effect-size (cohen's D)\n(mean difference divided by standard deviation). We already know that\nthe true effect-size is zero, however take a look at this graph, and pay\nclose attention to the smaller sample-sizes.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nall_df<-data.frame()\nfor(i in 1:1000){\n  for(n in c(10,20,50,100,1000)){\n    some_data<-rnorm(n,0,1)\n    p_value<-t.test(some_data)$p.value\n    effect_size<-mean(some_data)/sd(some_data)\n    mean_scores<-mean(some_data)\n    standard_error<-sd(some_data)/sqrt(length(some_data))\n    t_df<-data.frame(sim=i,sample_size=n,p_value,effect_size,mean_scores,standard_error)\n    all_df<-rbind(all_df,t_df)\n  }\n}\n\ntype_I_error <-all_df[all_df$p_value<.05,]\ntype_I_error$sample_size<-as.factor(type_I_error$sample_size)\n\nggplot(type_I_error,aes(x=p_value,y=effect_size, group=sample_size,color=sample_size))+\n  geom_point()+\n  theme_classic()+\n  ggtitle(\"Effect sizes for type I errors\")\n```\n\n::: {.cell-output-display}\n![Effect size as a function of p-values for type 1 Errors under the null, for a paired samples t-test.](12-Thinking_files/figure-pdf/fig-12effectsizeType1-1.pdf){#fig-12effectsizeType1 fig-pos='H' width=100%}\n:::\n:::\n\n\n\nFor example, look at the red dots, when sample size is 10. Here we see\nthat the effect-sizes are quite large. When p is near 0.05 the\neffect-size is around .8, and it goes up and up as when p gets smaller\nand smaller. What does this mean? It means that when you get unlucky\nwith a small N design, and your manipulation does not work, but you by\nchance find a \"significant\" effect, the effect-size measurement will\nshow you a \"big effect\". This is the pernicious aspect. When you make a\ntype I error for small N, your data will make you think there is no way\nit could be a type I error because the effect is just so big!. Notice\nthat when N is very large, like 1000, the measure of effect-size\napproaches 0 (which is the true effect-size in the simulation shown in\n@fig-12cohensD).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nall_df<-data.frame()\nfor(i in 1:10000){\n  sample      <- rnorm(50,100,20)\n  sample_mean <- mean(sample[1:25]-sample[26:50])\n  sample_sem  <- sd(sample)/sqrt(length(sample))\n  sample_t    <- t.test(sample, mu=100)$statistic\n  sample_d    <- (mean(sample)-100)/sd(sample)\n  t_df<-data.frame(i,sample_mean,\n                   sample_sem,\n                   sample_t,\n                   sample_d)\n  all_df<-rbind(all_df,t_df)\n}\n\nlibrary(ggpubr)\na<-ggplot(all_df,aes(x=sample_mean))+\n  geom_histogram(color=\"white\")+\n  theme_classic()\nb<-ggplot(all_df,aes(x=sample_sem))+\n  geom_histogram(color=\"white\")+\n  theme_classic()\nc<-ggplot(all_df,aes(x=sample_t))+\n  geom_histogram(color=\"white\")+\n  theme_classic()\nd<-ggplot(all_df,aes(x=sample_d))+\n  geom_histogram(color=\"white\")+\n  theme_classic()\n\nggarrange(a,b,c,d,\n          ncol = 2, nrow = 2)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n:::\n\n::: {.cell-output-display}\n![Each panel shows a histogram of a different sampling statistic.](12-Thinking_files/figure-pdf/fig-12cohensD-1.pdf){#fig-12cohensD fig-pos='H' width=100%}\n:::\n:::\n",
    "supporting": [
      "12-Thinking_files/figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}