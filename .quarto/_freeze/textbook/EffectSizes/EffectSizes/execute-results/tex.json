{
  "hash": "71391c18706221904055cf7ac83c465b",
  "result": {
    "markdown": "---\ntitle: \"Effect Sizes: Primer\"\nsubtitle: \"Wow! That had a huge effect!\"\n# image: \"lecture.png\"\nauthor: \n  - name: \"Dr. Gordon Wright\"\n    orcid: 0000-0001-9424-5743\n    email: g.wright@gold.ac.uk\n# date: \"10/09/2023\"\n# date-format: long\nlicense: \"CC BY-NC-SA\"\n\n# author: \"Dr. Gordon Wright & Professor Matthew J. C. Crump\"\n# aliases: [thinking-about-answering-questions-with-data.html]\n---\n\n\n\n\n# How much bang does my manipulation have?\n\n\n::: {.cell}\n\n:::\n\n\n## Effect-sizes\n\nAs you become Psychological Scientists at Goldsmiths, you are going to\nwant to go about the whole business properly. And good for you. We will\nbe talking about some of the crises Psychology has faced (and continues\nto face) over the course of the year. But the only way we can resolve\nsome of these issues is by training future Psychologists to be better\nthan those who have come before. I think you will very quickly notice\nthat published research often doesn't conform to the 'rules' we are\nteaching you. If you notice this... take that as evidence that you are\nwell on your way to being better than them!\n\nFirst, it is worth pointing out that over the years, at least in\nPsychology, many societies and journals have made strong recommendations\nabout how researchers should report their statistical analyses. Among\nthe recommendations is that measures of \"effect size\" should be\nreported. Similarly, many journals now require that researchers report\nan \"a priori\" power-analysis (the recommendation is this should be done\nbefore the data is collected). Because these recommendations are so\nprevalent, it is worth discussing what these ideas refer to. Indeed, you\nwill be identifying Effect Sizes and calculating them in your\nMini-Dissertations, and performing an a priori power calculation when\nyou submit an Ethics Application in both Year 2 and 3 for you\nDissertations.\n\nEffect Sizes and Power can seem confusing unless you are actually\n'doing' research or thinking about research in a more systematic way,\nconsidering multiple studies on a similar topic. This is why now is the\nideal time to deal with them. You are both actively researching, and\nlooking to summarise the results of multiple studies. Let's crack on.\n\nThe question or practice of using measures of effect size and conducting\npower-analyses are also good examples of the more general need to think\nabout about what you are doing. If you are going to report effect size,\nand conduct power analyses, these activities should not be done blindly\nbecause someone else recommends that you do them, these activities and\nother suitable ones should be done as a part of justifying what you are\ndoing. It is a part of thinking about how to make your data answer\nquestions for you. Matthew Crumps book (on which this content is based\nis called 'Answering Questions with Data' and is an excellent summary of\nthis process we call Science - see citation below)\n\n### Chance vs. real effects\n\nLet's cover a key point again. Primarily, researchers are interested in\nwhether their manipulation causes a change in their measurement. If it\ndoes, they can become confident that they have uncovered a causal force\n(the manipulation).\n\nHowever, we know that differences in the measure between experimental\nconditions can arise by chance alone, just by sampling error, or by dint\nof the people in two randomly allocated groups. In fact, we can create\npictures that show us the window of chance for a given statistic, these\ntell us roughly the range and likelihoods of getting various differences\njust by chance. With these windows in hand, we can then determine\nwhether the differences we found in some data that we collected were\nlikely or unlikely to be due to chance. We also learned that sample-size\nplays a big role in the shape of the 'chance window'. Small samples give\nchance a large opportunity to make big differences. Large samples give\nchance a smaller opportunity to make big differences. The general lesson\nup to this point has been, design an experiment with a large enough\nsample to detect the effect of interest. If your study isn't well\ndesigned, you could easily be measuring noise or random variation, and\nyour differences could be caused by sampling error or individual\nvariability. Generally speaking, this is still a very good lesson:\nbetter designs produce better data; and you can't fix a broken design\nafter the data are collected, e.g. by using statistics.\n\nBy running your Mini-Dissertation, and collecting real data early, we\nhope that this is one of the lessons you are able to learn. If you make\na mistake in the design of your study, you'll have to live with that all\nthe way through the process, and you'll kick yourself. That's why we\nencourage you to enjoy this first opportunity and to make some mistakes\nwith a smile. Those who do, will learn far more than those who don't!\nBut don't worry. Gordon will be trying to model good (and less good)\nresearch behaviour, so that you get lots of learning opportunities.\n\nThere is clearly another thing that can determine whether or not your\ndifferences are due to chance. That is the effect itself. If the\nmanipulation does cause a change, then there is an effect, and that\neffect is a real one. Effects refer to differences in the measurement\nbetween experimental conditions. The thing about effects is that they\ncan be big or small, they have a size.\n\nFor example, you can think of a manipulation in terms of the size of its\nhammer. A strong manipulation is like a jack-hammer: it is loud, it\nproduces a big effect, it creates huge differences. A medium\nmanipulation is like regular hammer: it works, you can hear it, it\ndrives a nail into wood, but it doesn't destroy concrete like a\njack-hammer, it produces a reliable effect. A small manipulation is like\ntapping something with a pencil: it does something, you can barely hear\nit, and only in a quiet room, it doesn't do a good job of driving a nail\ninto wood, and it does nothing to concrete, it produces tiny, unreliable\neffects. Finally, a really small effect would be hammering something\nwith a feather, it leaves almost no mark and does nothing that is\nobviously perceptible to nails or pavement. The lesson is, if you want\nto break up concrete, use a jack-hammer; or, if you want to measure your\neffect, make your manipulation stronger (like a jack-hammer) so it\nproduces a bigger difference that can be spotted easily. There is no\nprize for subtlety when it comes to an experimental manipulation. If in\ndoubt, give it some clout!\n\n### Effect size: concrete vs. abstract notions\n\nGenerally speaking, the big concept of effect size, is simply how big\nthe differences are between people who don't get your manipulation, and\nthose that do, that's it. However, the biggness or smallness of effects\nquickly becomes a little bit complicated. On the one hand, the raw\ndifference in the means can be very meaningful. Let's say we are\nmeasuring performance on a final exam, and we are testing whether or not\na miracle drug can make you do better on the test. Let's say taking the\ndrug makes you do 5% better on the test, compared to not taking the\ndrug. You know what 5% means, that's basically a whole category higher.\nPretty good. An effect-size of 25% would be even better right! Lot's of\nmeasures have a concrete quality to them, and it makes sense to state\nthe size of the effect expressed in terms of the original measure.\n\nLet's talk about concrete measures some more. How about learning a\nmusical instrument. Let's say it takes 10,000 hours to become an expert\npiano, violin, or guitar player. And, let's say you found something\nonline that says that using their method, you will learn the instrument\nin `less time` than normal. That is a claim about the effect size of\ntheir method. You would want to know how big the effect is right? For\nexample, the effect-size could be 10 hours. That would mean it would\ntake you 9,980 hours to become an expert (that's a whole 10 hours less).\nIf I knew the effect-size was so tiny, I wouldn't bother with their new\nmethod, I might reason I could make that sort of effect by saving my\nmoney and using it to buy snacks. But, if the effect size was say 1,000\nhours, that's a pretty big deal, that's 10% less (still doesn't seem\nlike much, but saving 1,000 hours seems like a lot).\n\nJust as often as we have concrete measures that are readily\ninterpretable (hours, percentages, snack-equivalents), Psychology often\nproduces measures that are extremely difficult to interpret. For\nexample, questionnaire measures often have no concrete meaning, and only\nan abstract statistical meaning. If you wanted to know whether a\nmanipulation caused people to be more or less happy, and you used a\nquestionnaire to measure happiness in `happy units`, you might find that\npeople were `50 happy` in condition 1, and `60 happy` in condition 2,\nthat's a difference of `10 happy units`. But how much is 10? Is that a\nbig or small difference? A smile to a toothy smile? ROFL? It's not\nimmediately obvious. What is the solution here? A common solution is to\nprovide a standardized measure of the difference, like a z-score. For\nexample, if a difference of 10 reflected a shift of one standard\ndeviation that would be useful to know, and that would be a sizeable\nshift. If the difference was only a tenth of the naturally occurring\nstandard deviation, then the difference of 10 wouldn't be very large at\nall, you'd probably have difficulty spotting it in the wild. We\nelaborate on this idea next in describing cohen's d.\n\n### Cohen's d\n\nLet's look a few distributions to firm up some ideas about effect-size.\n@fig-12effectdists has four panels. The first panel (0) represents the\nnull distribution of no differences. This is the idea that your\nmanipulation (A vs. B) doesn't do anything at all, as a result when you\nmeasure scores in conditions A and B, you are effectively sampling\nscores from the very same overall distribution. The panel shows the\ndistribution as green for condition B, but the red one for condition A\nis identical and drawn underneath (it's invisible). There is 0\ndifference between these distributions, so it represents a null effect.\nZilch.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Each panel shows hypothetical distributions for two conditions. As the effect-size increases, the difference between the distributions become larger.](EffectSizes_files/figure-pdf/fig-12effectdists-1.pdf){#fig-12effectdists width=100%}\n:::\n:::\n\n\nThe remaining panels are hypothetical examples of what a true effect\ncould look like, when your manipulation actually causes a difference.\nFor example, if condition A is a control group, and condition B is a\ntreatment group (our magic study drug), we are looking at three cases\nwhere the treatment manipulation causes a positive shift in the mean of\nthe treatment group distribution. We are using normal curves with mean\n=0 and sd =1 for this demonstration, so a shift of .5 is a shift of half\nof a standard deviation. A shift of 1 is a shift of 1 standard\ndeviation, and a shift of 2 is a shift of 2 standard deviations. We\ncould draw many more examples showing even bigger shifts, or shifts that\ngo in the other direction.\n\nLet's look at another example, but this time we'll use some concrete\nmeasurements. Let's say we are looking at final exam performance, so our\nnumbers are grade percentages. Let's also say that we know the mean on\nthe test is 65%, with a standard deviation of 5%. Group A could be a\ncontrol that just takes the test, Group B could receive some\n\"educational\" manipulation designed to improve the test score. These\ngraphs then show us some hypotheses about what the manipulation may or\nmay not be doing.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Each panel shows hypothetical distributions for two conditions. As the effect-size increases, the difference between the distributions become larger.](EffectSizes_files/figure-pdf/fig-12effectdistsB-1.pdf){#fig-12effectdistsB width=100%}\n:::\n:::\n\n\nThe first panel shows that both condition A and B will sample test\nscores from the same distribution (mean =65, with 0 effect). The other\npanels show shifted mean for condition B (the treatment that is supposed\nto increase test performance). So, the treatment could increase the test\nperformance by 2.5% (mean 67.5, .5 sd shift), or by 5% (mean 70, 1 sd\nshift), or by 10% (mean 75%, 2 sd shift), or by any other amount. In\nterms of our previous metaphor, a shift of 2 standard deviations is more\nlike a jack-hammer in terms of size, and a shift of .5 standard\ndeviations is more like using a pencil. The thing about research, is we\noften have no clue about whether our manipulation will produce a big or\nsmall effect, that's why we are conducting the research - to find out!\n\nYou might have noticed that the letter $d$ appears in the above figure.\nWhy is that? Jacob Cohen @Cohen1988 used the letter $d$ in defining the\neffect-size for this situation, and now everyone calls it Cohen's $d$.\nThe formula for Cohen's $d$ is below (no need to memorise this!):\n\n$d = \\frac{\\text{mean for condition 1} - \\text{mean for condition 2}}{\\text{population standard deviation}}$\n\nIf you notice, this is just a kind of z-score. It is a way to\nstandardize the mean difference in terms of the population standard\ndeviation. It uses the Standard Deviation as a unit of measurement.\n\nIt is also worth noting again that this measure of effect-size is\nentirely hypothetical for most purposes. In general, researchers do not\nknow the population standard deviation, they can only guess at it, or\nestimate it from the sample. The same goes for means, in the formula\nthese are hypothetical mean differences in two population distributions.\nIn practice, researchers do not know these values, they guess at them\nfrom their samples. When we talk about replication efforts, we will\nconsider this. For example, if I try to replicate an experiment\npublished in a journal article, and do an amazing job, but my\nparticipants are fundamentally different, the performance may be\ndifferent across the board, and although the effect is consistent, the\nresults may look different due to the type of people tested.\n\nBefore discussing why the concept of effect-size can be useful, we note\nthat Cohen's $d$ is useful for understanding abstract measures. For\nexample, when you don't know what a difference of 10 or 20 means as a\nraw score, you can standardize the difference by the sample standard\ndeviation, then you know roughly how big the effect is in terms of\nstandard units. If you thought a \\`20 happys\\` was big, but it turned\nout to be only 1/10th of a standard deviation, then you would know the\neffect is actually quite small with respect to the overall variability\nin the data.\n\n## Acknowledgments {.appendix}\n\nThis content was 'remixed' by Dr. Gordon Wright and built upon fantastic\ncontent created by Professor Matthew J.C. Crump and shared under CC BY\nSA 4.0. Prof Crump... You were among the really cool Psychologists who\nmade me give this whole Quarto, OER stuff a go. I'd like to buy you a\nbeer.\n\nThe original content is available here\nhttps://www.crumplab.com/statistics/12-Thinking.html\n\nCitation: Crump, M. J. C., Navarro, D. J., & Suzuki, J. (2019, June 5).\nAnswering Questions with Data (Textbook): Introductory Statistics for\nPsychology Students. https://doi.org/10.17605/OSF.IO/JZE52\n\n## References\n",
    "supporting": [
      "EffectSizes_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}