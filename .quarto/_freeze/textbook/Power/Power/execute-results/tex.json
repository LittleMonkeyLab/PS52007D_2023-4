{
  "hash": "91e079a113c6eb8a585be5f8864bfb63",
  "result": {
    "markdown": "---\ntitle: \"Power!\"\nsubtitle: \"Wow! That had a huge effect!\"\n# image: \"lecture.png\"\nauthor: \n  - name: \"Dr. Gordon Wright\"\n    orcid: 0000-0001-9424-5743\n    email: g.wright@gold.ac.uk\n# date: \"10/09/2023\"\n# date-format: long\nlicense: \"CC BY-NC-SA\"\n\n\n# author: \"Dr. Gordon Wright & Professor Matthew J. C. Crump\"\n---\n\n::: {.cell}\n\n:::\n\n\n## Power\n\nWhen there is a true effect out there to measure, you want to make sure\nyour design is sensitive enough to detect the effect, otherwise what's\nthe point. We've already talked about the idea that an effect can have\ndifferent sizes. The next idea is that your design can be more less\nsensitive in its ability to reliabily measure the effect. We have\ndiscussed this general idea many times already in the textbook, for\nexample we know that we will be more likely to detect \"significant\"\neffects (when there are real differences) when we increase our\nsample-size. Here, we will talk about the idea of design sensitivity in\nterms of the concept of power. Interestingly, the concept of power is a\nsomewhat limited concept, in that it only exists as a concept within\nsome philosophies of statistics.\n\n### A digresssion about hypothesis testing\n\nIn particular, the concept of power falls out of the Neyman-Pearson\nconcept of null vs. alternative hypothesis testing. Up to this point, we\nhave largely avoided this terminology. This is perhaps a disservice in\nthat the Neyman-Pearson ideas are by now the most common and widespread,\nand in the opinion of some of us, they are also the most widely\nmisunderstood and abused idea, which is why we have avoided these ideas\nuntil now.\n\nWhat we have been mainly doing is talking about hypothesis testing from\nthe Fisherian (Sir Ronald Fisher, the ANOVA guy) perspective. This is a\nbasic perspective that we think can't be easily ignored. It is also\nquite limited. The basic idea is this:\n\n1.  We know that chance can cause some differences when we measure\n    something between experimental conditions.\n2.  We want to rule out the possibility that the difference that we\n    observed can not be due to chance\n3.  We construct large N designs that permit us to do this when a real\n    effect is observed, such that we can confidently say that big\n    differences that we find are so big (well outside the chance window)\n    that it is highly implausible that chance alone could have produced.\n4.  The final conclusion is that chance was extremely unlikely to have\n    produced the differences. We then infer that something else, like\n    the manipulation, must have caused the difference.\n5.  We don't say anything else about the something else.\n6.  We either reject the null distribution as an explanation (that\n    chance couldn't have done it), or retain the null (admit that chance\n    could have done it, and if it did we couldn't tell the difference\n    between what we found and what chance could do)\n\nNeyman and Pearson introduced one more idea to this mix, the idea of an\nalternative hypothesis. The alternative hypothesis is the idea that if\nthere is a true effect, then the data sampled into each condition of the\nexperiment must have come from two different distributions. Remember,\nwhen there is no effect we assume all of the data cam from the same\ndistribution (which by definition can't produce true differences in the\nlong run, because all of the numbers are coming from the same\ndistribution). The graphs of effect-sizes from before show examples of\nthese alternative distributions, with samples for condition A coming\nfrom one distribution, and samples from condition B coming from a\nshifted distribution with a different mean.\n\nSo, under the Neyman-Pearson tradition, when a researcher find a\nsignifcant effect they do more than one things. First, they reject the\nnull-hypothesis of no differences, and they accept the alternative\nhypothesis that there was differences. This seems like a sensible thing\nto do. And, because the researcher is actually interested in the\nproperties of the real effect, they might be interested in learning more\nabout the actual alternative hypothesis, that is they might want to know\nif their data come from two different distributions that were separated\nby some amount...in other words, they would want to know the size of the\neffect that they were measuring.\n\n### Back to power\n\nWe have now discussed enough ideas to formalize the concept of\nstatistical power. For this concept to exist we need to do a couple\nthings.\n\n1.  Agree to set an alpha criterion. When the p-value for our\n    test-statistic is below this value we will call our finding\n    statistically significant, and agree to reject the null hypothesis\n    and accept the \"alternative\" hypothesis (sidenote, usually it isn't\n    very clear which specific alternative hypothesis was accepted)\n2.  In advance of conducting the study, figure out what kinds of\n    effect-sizes our design is capable of detecting with particular\n    probabilites.\n\nThe power of a study is determined by the relationship between\n\n1.  The sample-size of the study\n2.  The effect-size of the manipulation\n3.  The alpha value set by the researcher.\n\nTo see this in practice let's do a simulation. We will do a t-test on a\nbetween-groups design 10 subjects in each group. Group A will be a\ncontrol group with scores sampled from a normal distribution with mean\nof 10, and standard deviation of 5. Group B will be a treatment group,\nwe will say the treatment has an effect-size of Cohen's $d$ = .5, that's\na standard deviation shift of .5, so the scores with come from a normal\ndistribution with mean =12.5 and standard deivation of 5. Remember 1\nstandard deviation here is 5, so half of a standard deviation is 2.5.\n\nThe following R script runs this simulated experiment 1000 times. We set\nthe alpha criterion to .05, this means we will reject the null whenever\nthe $p$-value is less than .05. With this specific design, how many\ntimes out of of 1000 do we reject the null, and accept the alternative\nhypothesis?\n\n\n::: {.cell}\n\n```{.r .cell-code}\np<-length(1000)\nfor(i in 1:1000){\n  A<-rnorm(10,10,5)\n  B<-rnorm(10,12.5,5)\n  p[i]<-t.test(A,B,var.equal = TRUE)$p.value\n}\n\nlength(p[p<.05])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 180\n```\n:::\n:::\n\n\nThe answer is that we reject the null, and accept the alternative\n180 times out of 1000. In other words our experiment\nsuccesfully accepts the alternative hypothesis\n18 percent of the time, this is known as the\npower of the study. Power is the probability that a design will\nsuccesfully detect an effect of a specific size.\n\nImportantly, power is completely abstract idea that is completely\ndetermined by many assumptions including N, effect-size, and alpha. As a\nresult, it is best not to think of power as a single number, but instead\nas a family of numbers.\n\nFor example, power is different when we change N. If we increase N, our\nsamples will more precisely estimate the true distributions that they\ncame from. Increasing N reduces sampling error, and shrinks the range of\ndifferences that can be produced by chance. Lets' increase our N in this\nsimulation from 10 to 20 in each group and see what happens.\n\n\n::: {.cell}\n\n```{.r .cell-code}\np<-length(1000)\nfor(i in 1:1000){\n  A<-rnorm(20,10,5)\n  B<-rnorm(20,12.5,5)\n  p[i]<-t.test(A,B,var.equal = TRUE)$p.value\n}\n\nlength(p[p<.05])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 342\n```\n:::\n:::\n\n\nNow the number of significant experiments i 342 out of\n1000, or a power of 34.2 percent. That's\nroughly doubled from before. We have made the design more sensitive to\nthe effect by increasing N.\n\nWe can change the power of the design by changing the alpha-value, which\ntells us how much evidence we need to reject the null. For example, if\nwe set the alpha criterion to 0.01, then we will be more conservative,\nonly rejecting the null when chance can produce the observed difference\n1% of the time. In our example, this will have the effect of reducing\npower. Let's keep N at 20, but reduce the alpha to 0.01 and see what\nhappens:\n\n\n::: {.cell}\n\n```{.r .cell-code}\np<-length(1000)\nfor(i in 1:1000){\n  A<-rnorm(20,10,5)\n  B<-rnorm(20,12.5,5)\n  p[i]<-t.test(A,B,var.equal = TRUE)$p.value\n}\n\nlength(p[p<.01])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 146\n```\n:::\n:::\n\n\nNow only 146 out of 1000 experiments are significant,\nthat's 14.6 power.\n\nFinally, the power of the design depends on the actual size of the\neffect caused by the manipulation. In our example, we hypothesized that\nthe effect caused a shift of .5 standard deviations. What if the effect\ncauses a bigger shift? Say, a shift of 2 standard deviations. Let's keep\nN= 20, and alpha \\< .01, but change the effect-size to two standard\ndeviations. When the effect in the real-world is bigger, it should be\neasier to measure, so our power will increase.\n\n\n::: {.cell}\n\n```{.r .cell-code}\np<-length(1000)\nfor(i in 1:1000){\n  A<-rnorm(20,10,5)\n  B<-rnorm(20,30,5)\n  p[i]<-t.test(A,B,var.equal = TRUE)$p.value\n}\n\nlength(p[p<.01])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1000\n```\n:::\n:::\n\n\nNeat, if the effect-size is actually huge (2 standard deviation shift),\nthen we have power 100 percent to detect the\ntrue effect.\n\n### Power curves\n\nWe mentioned that it is best to think of power as a family of numbers,\nrather than as a single number. To elaborate on this consider the power\ncurve below. This is the power curve for a specific design: a between\ngroups experiments with two levels, that uses an independent samples\nt-test to test whether an observed difference is due to chance.\nCritically, N is set to 10 in each group, and alpha is set to .05\n\nIn @fig-12powercurve power (as a proportion, not a percentage) is\nplotted on the y-axis, and effect-size (Cohen's d) in standard deviation\nunits is plotted on the x-axis.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npower<-c()\nfor(i in seq(0,2,.1)){\nsd_AB <- 1\nn<-10\nC <- qnorm(0.975)\nse <- sqrt( sd_AB/n + sd_AB/n )\ndelta<-i\npower <- c(power,1-pnorm(C-delta/se) + pnorm(-C-delta/se))\n}\n\nplot_df<-data.frame(power,\n                    effect_size = seq(0,2,.1))\n\nggplot(plot_df, aes(x=effect_size, y=power))+\n  geom_line()+\n  theme_classic()\n```\n\n::: {.cell-output-display}\n![This figure shows power as a function of effect-size (Cohen's d) for a between-subjects independent samples t-test, with N=10, and alpha criterion 0.05.](Power_files/figure-pdf/fig-12powercurve-1.pdf){#fig-12powercurve fig-pos='H' width=100%}\n:::\n:::\n\n\nA power curve like this one is very helpful to understand the\nsensitivity of a particular design. For example, we can see that a\nbetween subjects design with N=10 in both groups, will detect an effect\nof d=.5 (half a standard deviation shift) about 20% of the time, will\ndetect an effect of d=.8 about 50% of the time, and will detect an\neffect of d=2 about 100% of the time. All of the percentages reflect the\npower of the design, which is the percentage of times the design would\nbe expected to find a $p$ \\< 0.05.\n\nLet's imagine that based on prior research, the effect you are\ninterested in measuring is fairly small, d=0.2. If you want to run an\nexperiment that will detect an effect of this size a large percentage of\nthe time, how many subjects do you need to have in each group? We know\nfrom the above graph that with N=10, power is very low to detect an\neffect of d=0.2. Let's make @fig-12powercurveN and vary the number of\nsubjects rather than the size of the effect.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npower<-c()\nfor(i in seq(10,800,10)){\nsd_AB <- 1\nn<-i\nC <- qnorm(0.975)\nse <- sqrt( sd_AB/n + sd_AB/n )\ndelta<-0.2\npower <- c(power,1-pnorm(C-delta/se) + pnorm(-C-delta/se))\n}\n\nplot_df<-data.frame(power,\n                    N = seq(10,800,10))\n\nggplot(plot_df, aes(x=N, y=power))+\n  geom_line()+\n  theme_classic()+\n  geom_hline(yintercept=.8, color=\"green\")\n```\n\n::: {.cell-output-display}\n![This figure shows power as a function of N for a between-subjects independent samples t-test, with d=0.2, and alpha criterion 0.05.](Power_files/figure-pdf/fig-12powercurveN-1.pdf){#fig-12powercurveN fig-pos='H' width=100%}\n:::\n:::\n\n\nThe figure plots power to detect an effect of d=0.2, as a function of N.\nThe green line shows where power = .8, or 80%. It looks like we would\nnee about 380 subjects in each group to measure an effect of d=0.2, with\npower = .8. This means that 80% of our experiments would succesfully\nshow p \\< 0.05. Often times power of 80% is recommended as a reasonable\nlevel of power, however even when your design has power = 80%, your\nexperiment will still fail to find an effect (associated with that level\nof power) 20% of the time!\n\n## Planning your design\n\nOur discussion of effect size and power highlight the importance of the\nunderstanding the statistical limitations of an experimental design. In\nparticular, we have seen the relationship between:\n\n1.  Sample-size\n2.  Effect-size\n3.  Alpha criterion\n4.  Power\n\nAs a general rule of thumb, small N designs can only reliably detect\nvery large effects, whereas large N designs can reliably detect much\nsmaller effects. As a researcher, it is your responsibility to plan your\ndesign accordingly so that it is capable of reliably detecting the kinds\nof effects it is intended to measure.\n\n## Some considerations\n\n### Low powered studies\n\nConsider the following case. A researcher runs a study to detect an\neffect of interest. There is good reason, from prior research, to\nbelieve the effect-size is d=0.5. The researcher uses a design that has\n30% power to detect the effect. They run the experiment and find a\nsignificant p-value, (p\\<.05). They conclude their manipulation worked,\nbecause it was unlikely that their result could have been caused by\nchance. How would you interpret the results of a study like this? Would\nyou agree with thte researchers that the manipulation likely caused the\ndifference? Would you be skeptical of the result?\n\nThe situation above requires thinking about two kinds of probabilities.\nOn the one hand we know that the result observed by the researchers does\nnot occur often by chance (p is less than 0.05). At the same time, we\nknow that the design was underpowered, it only detects results of the\nexpected size 30% of the time. We are face with wondering what kind of\nluck was driving the difference. The researchers could have gotten\nunlucky, and the difference really could be due to chance. In this case,\nthey would be making a type I error (saying the result is real when it\nisn't). If the result was not due to chance, then they would also be\nlucky, as their design only detects this effect 30% of the time.\n\nPerhaps another way to look at this situation is in terms of the\nreplicability of the result. Replicability refers to whether or not the\nfindings of the study would be the same if the experiment was repeated.\nBecause we know that power is low here (only 30%), we would expect that\nmost replications of this experiment would not find a significant\neffect. Instead, the experiment would be expected to replicate only 30%\nof the time.\n\n### Large N and small effects\n\nPerhaps you have noticed that there is an intriguiing relationship\nbetween N (sample-size) and power and effect-size. As N increases, so\ndoes power to detect an effect of a particular size. Additionally, as N\nincreases, a design is capable of detecting smaller and smaller effects\nwith greater and greater power. For example, if N was large enough, we\nwould have high power to detect very small effects, say d= 0.01, or even\nd=0.001. Let's think about what this means.\n\nImagine a drug company told you that they ran an experiment with 1\nbillion people to test whether their drug causes a significant change in\nheadache pain. Let's say they found a significant effect (with power\n=100%), but the effect was very small, it turns out the drug reduces\nheadache pain by less than 1%, let's say 0.01%. For our imaginary study\nwe will also assume that this effect is very real, and not caused by\nchance.\n\nClearly the design had enough power to detect the effect, and the effect\nwas there, so the design did detect the effect. However, the issue is\nthat there is little practical value to this effect. Nobody is going to\nby a drug to reduce their headache pain by 0.01%, even if it was\n\"scientifcally proven\" to work. This example brings up two issues.\nFirst, increasing N to very large levels will allow designs to detect\nalmost any effect (even very tiny ones) with very high power. Second,\nsometimes effects are meaningless when they are very small, especially\nin applied research such as drug studies.\n\nThese two issues can lead to interesting suggestions. For example,\nsomeone might claim that large N studies aren't very useful, because\nthey can always detect really tiny effects that are practically\nmeaningless. On the other hand, large N studies will also detect larger\neffects too, and they will give a better estimate of the \"true\" effect\nin the population (because we know that larger samples do a better job\nof estimating population parameters). Additionally, although really\nsmall effects are often not interesting in the context of applied\nresearch, they can be very important in theoretical research. For\nexample, one theory might predict that manipulating X should have no\neffect, but another theory might predict that X does have an effect,\neven if it is a small one. So, detecting a small effect can have\ntheoretical implication that can help rule out false theories. Generally\nspeaking, researchers asking both theoretical and applied questions\nshould think about and establish guidelines for \"meaningful\"\neffect-sizes so that they can run designs of appropriate size to detect\neffects of \"meaningful size\".\n\n### Small N and Large effects\n\nAll other things being equal would you trust the results from a study\nwith small N or large N? This isn't a trick question, but sometimes\npeople tie themselves into a knot trying to answer it. We already know\nthat large sample-sizes provide better estimates of the distributions\nthe samples come from. As a result, we can safely conclude that we\nshould trust the data from large N studies more than small N studies.\n\nAt the same time, you might try to convince yourself otherwise. For\nexample, you know that large N studies can detect very small effects\nthat are practically and possibly even theoretically meaningless. You\nalso know that that small N studies are only capable of reliably\ndetecting very large effects. So, you might reason that a small N study\nis better than a large N study because if a small N study detects an\neffect, that effect must be big and meaningful; whereas, a large N study\ncould easily detect an effect that is tiny and meaningless.\n\nThis line of thinking needs some improvement. First, just because a\nlarge N study can detect small effects, doesn't mean that it only\ndetects small effects. If the effect is large, a large N study will\neasily detect it. Large N studies have the power to detect a much wider\nrange of effects, from small to large. Second, just because a small N\nstudy detected an effect, does not mean that the effect is real, or that\nthe effect is large. For example, small N studies have more variability,\nso the estimate of the effect size will have more error. Also, there is\n5% (or alpha rate) chance that the effect was spurious. Interestingly,\nthere is a pernicious relationship between effect-size and type I error\nrate\n\n### Type I errors are convincing when N is small\n\nSo what is this pernicious relationship between Type I errors and\neffect-size? Mainly, this relationship is pernicious for small N\nstudies. For example, the following figure illustrates the results of\n1000s of simulated experiments, all assuming the null distribution. In\nother words, for all of these simulations there is no true effect, as\nthe numbers are all sampled from an identical distribution (normal\ndistribution with mean =0, and standard deviation =1). The true\neffect-size is 0 in all cases.\n\nWe know that under the null, researchers will find p values that are\nless 5% about 5% of the time, remember that is the definition. So, if a\nresearcher happened to be in this situation (where there manipulation\ndid absolutely nothing), they would make a type I error 5% of the time,\nor if they conducted 100 experiments, they would expect to find a\nsignificant result for 5 of them.\n\n@fig-12effectsizeType1 reports the findings from only the type I errors,\nwhere the simulated study did produce p \\< 0.05. For each type I error,\nwe calculated the exact p-value, as well as the effect-size (cohen's D)\n(mean difference divided by standard deviation). We already know that\nthe true effect-size is zero, however take a look at this graph, and pay\nclose attention to the smaller sample-sizes.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nall_df<-data.frame()\nfor(i in 1:1000){\n  for(n in c(10,20,50,100,1000)){\n    some_data<-rnorm(n,0,1)\n    p_value<-t.test(some_data)$p.value\n    effect_size<-mean(some_data)/sd(some_data)\n    mean_scores<-mean(some_data)\n    standard_error<-sd(some_data)/sqrt(length(some_data))\n    t_df<-data.frame(sim=i,sample_size=n,p_value,effect_size,mean_scores,standard_error)\n    all_df<-rbind(all_df,t_df)\n  }\n}\n\ntype_I_error <-all_df[all_df$p_value<.05,]\ntype_I_error$sample_size<-as.factor(type_I_error$sample_size)\n\nggplot(type_I_error,aes(x=p_value,y=effect_size, group=sample_size,color=sample_size))+\n  geom_point()+\n  theme_classic()+\n  ggtitle(\"Effect sizes for type I errors\")\n```\n\n::: {.cell-output-display}\n![Effect size as a function of p-values for type 1 Errors under the null, for a paired samples t-test.](Power_files/figure-pdf/fig-12effectsizeType1-1.pdf){#fig-12effectsizeType1 fig-pos='H' width=100%}\n:::\n:::\n\n\nFor example, look at the red dots, when sample size is 10. Here we see\nthat the effect-sizes are quite large. When p is near 0.05 the\neffect-size is around .8, and it goes up and up as when p gets smaller\nand smaller. What does this mean? It means that when you get unlucky\nwith a small N design, and your manipulation does not work, but you by\nchance find a \"significant\" effect, the effect-size measurement will\nshow you a \"big effect\". This is the pernicious aspect. When you make a\ntype I error for small N, your data will make you think there is no way\nit could be a type I error because the effect is just so big!. Notice\nthat when N is very large, like 1000, the measure of effect-size\napproaches 0 (which is the true effect-size in the simulation shown in\n@fig-12cohensD).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nall_df<-data.frame()\nfor(i in 1:10000){\n  sample      <- rnorm(50,100,20)\n  sample_mean <- mean(sample[1:25]-sample[26:50])\n  sample_sem  <- sd(sample)/sqrt(length(sample))\n  sample_t    <- t.test(sample, mu=100)$statistic\n  sample_d    <- (mean(sample)-100)/sd(sample)\n  t_df<-data.frame(i,sample_mean,\n                   sample_sem,\n                   sample_t,\n                   sample_d)\n  all_df<-rbind(all_df,t_df)\n}\n\nlibrary(ggpubr)\na<-ggplot(all_df,aes(x=sample_mean))+\n  geom_histogram(color=\"white\")+\n  theme_classic()\nb<-ggplot(all_df,aes(x=sample_sem))+\n  geom_histogram(color=\"white\")+\n  theme_classic()\nc<-ggplot(all_df,aes(x=sample_t))+\n  geom_histogram(color=\"white\")+\n  theme_classic()\nd<-ggplot(all_df,aes(x=sample_d))+\n  geom_histogram(color=\"white\")+\n  theme_classic()\n\nggarrange(a,b,c,d,\n          ncol = 2, nrow = 2)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n:::\n\n::: {.cell-output-display}\n![Each panel shows a histogram of a different sampling statistic.](Power_files/figure-pdf/fig-12cohensD-1.pdf){#fig-12cohensD fig-pos='H' width=100%}\n:::\n:::\n\n\n## Acknowledgements\n\nMany thanks to Professor Matthew J. C. Crump for the original draft of\nthis content, remixed under CC BY-SA 4.0 by Dr. Gordon Wright. Original\ncontent available here\nhttps://www.crumplab.com/statistics/12-Thinking.html#power\n",
    "supporting": [
      "Power_files/figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}